{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Scrape\n",
    "Useful tutorial: https://www.youtube.com/watch?v=XjNm9bazxn8&index=5&list=WL  \n",
    "\n",
    "Target site(s):\n",
    "http://www.fixmystreet.org.au/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting url.\n",
    "base_url = 'http://www.fixmystreet.org.au/reports'\n",
    "start_url = 'http://www.fixmystreet.org.au/reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the top level links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links that contain '/NVRSHIPS/HULL_'.\n",
    "    links_list = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        try:\n",
    "            if exclude_string == None:\n",
    "                if target_string in link.get('href'):\n",
    "                    links_list.append(link.get('href'))\n",
    "            elif exclude_string != None:\n",
    "                if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                    links_list.append(link.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    \"\"\"Return a dictionary of info for the requested URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains scraped ship info with key = ship name, and values as ship info.\n",
    "    \n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    id_num = re.search('(\\d+$)', url).group(1)\n",
    "    try:\n",
    "        title = soup.find('h1', attrs={'class':'moderate-display'}).get_text()\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        category_raw = soup.find('p', attrs={'class':'report_meta_info'}).get_text()\n",
    "        category = re.search('in the (.+) category', category_raw).group(1)\n",
    "    except:\n",
    "        category = None\n",
    "    \n",
    "    try:\n",
    "        comment_raw = soup.find('div', attrs={\"class\":\"moderate-display\"}).get_text()\n",
    "        comment = re.search('\\\\n(.+)\\\\n', comment_raw).group(1)\n",
    "    except:\n",
    "        comment = None\n",
    "        \n",
    "    info[id_num] = {'id': id_num,\n",
    "                    'title': title,\n",
    "                    'category': category,\n",
    "                    'comment': comment\n",
    "                   }\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1399</td>\n",
       "      <td>Street Light Out</td>\n",
       "      <td>General</td>\n",
       "      <td>The street light is out... again.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id             title category                            comment\n",
       "0  1399  Street Light Out  General  The street light is out... again."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_li(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the second level links.\n",
    "    Recursively run if pagination found.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links that contain '/NVRSHIPS/HULL_'.\n",
    "    links_list = []\n",
    "    \n",
    "    # Look for additional pages and recursively call.\n",
    "    if soup.find('a', {'class': 'next'}):\n",
    "        links_list += links_li(soup.find('a', {'class': 'next'}).get('href'), target_string, exclude_string)\n",
    "    \n",
    "    for li_item in soup.find_all('li', {'class': 'item-list__item item-list--reports__item '}):\n",
    "        for link in li_item.find_all('a'):\n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return links_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "4.71 min elapsed\n",
      "200\n",
      "10.51 min elapsed\n",
      "300\n",
      "15.65 min elapsed\n",
      "400\n",
      "22.58 min elapsed\n",
      "500\n",
      "29.23 min elapsed\n",
      "600\n",
      "35.36 min elapsed\n",
      "700\n",
      "40.69 min elapsed\n",
      "800\n",
      "47.10 min elapsed\n",
      "900\n",
      "52.02 min elapsed\n",
      "1000\n",
      "60.25 min elapsed\n",
      "1100\n",
      "65.94 min elapsed\n",
      "1200\n",
      "72.41 min elapsed\n",
      "Completed download of 1247 records in 75.12 minutes!\n"
     ]
    }
   ],
   "source": [
    "# Main scraping loop.\n",
    "# Requires top_level_links above.\n",
    "\n",
    "info = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "top_level_links = links(start_url, 'reports/', '?')\n",
    "\n",
    "for top_link in top_level_links:\n",
    "    # Grab next level links.\n",
    "    second_level_links = links_li(top_link, 'report/')\n",
    "    \n",
    "#     # TEST\n",
    "#     print(second_level_links)\n",
    "#     break\n",
    "    \n",
    "    # Go to each link.\n",
    "    for second_link in second_level_links:\n",
    "        scraped_info = scraper(second_link) # dict\n",
    "        info.update(scraped_info) # Merges dict\n",
    "        \n",
    "        # Take a break to not hammer the site.\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            print('{:.2f} min elapsed'.format((time.time() - start_time)/ 60))\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print('Completed download of {} records in {:.2f} minutes!'.format(count, (time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "current_datetime = datetime.datetime.now()\n",
    "output_name = 'fixmystreet_list_au_' + current_datetime.strftime(\"%Y-%m-%d_%H-%M\") + '.csv'\n",
    "\n",
    "pd.DataFrame.from_dict(info, orient='index').reset_index(drop=True).to_csv(output_name, index_label='index')\n",
    "# pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
