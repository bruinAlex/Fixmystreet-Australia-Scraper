{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Scrape\n",
    "Useful tutorial: https://www.youtube.com/watch?v=XjNm9bazxn8&index=5&list=WL  \n",
    "\n",
    "Target site(s):\n",
    "http://www.fixmystreet.org.au/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting url.\n",
    "base_url = 'http://www.fixmystreet.org.au/reports'\n",
    "start_url = 'http://www.fixmystreet.org.au/reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the top level links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links that contain '/NVRSHIPS/HULL_'.\n",
    "    links_list = []\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        try:\n",
    "            if exclude_string == None:\n",
    "                if target_string in link.get('href'):\n",
    "                    links_list.append(link.get('href'))\n",
    "            elif exclude_string != None:\n",
    "                if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                    links_list.append(link.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    \"\"\"Return a dictionary of info for the requested URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains scraped ship info with key = ship name, and values as ship info.\n",
    "    \n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    id_num = re.search('(\\d+$)', url).group(1)\n",
    "    try:\n",
    "        title = soup.find('h1', attrs={'class':'moderate-display'}).get_text()\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        category_raw = soup.find('p', attrs={'class':'report_meta_info'}).get_text()\n",
    "        category = re.search('in the (.+) category', category_raw).group(1)\n",
    "    except:\n",
    "        category = None\n",
    "    \n",
    "    try:\n",
    "        comment_raw = soup.find('div', attrs={\"class\":\"moderate-display\"}).get_text()\n",
    "        comment = re.search('\\\\n(.+)\\\\n', comment_raw).group(1)\n",
    "    except:\n",
    "        comment = None\n",
    "        \n",
    "    info[id_num] = {'id': id_num,\n",
    "                    'title': title,\n",
    "                    'category': category,\n",
    "                    'comment': comment\n",
    "                   }\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_li(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the second level links.\n",
    "    Recursively run if pagination found.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links that contain '/NVRSHIPS/HULL_'.\n",
    "    links_list = []\n",
    "    \n",
    "    # Look for additional pages and recursively call.\n",
    "    if soup.find('a', {'class': 'next'}):\n",
    "        links_list += links_li(soup.find('a', {'class': 'next'}).get('href'), target_string, exclude_string)\n",
    "    \n",
    "    for li_item in soup.find_all('li', {'class': 'item-list__item item-list--reports__item '}):\n",
    "        for link in li_item.find_all('a'):\n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return links_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main scraping loop.\n",
    "# Requires top_level_links above.\n",
    "\n",
    "info = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "top_level_links = links(start_url, 'reports/', '?')\n",
    "\n",
    "for top_link in top_level_links:\n",
    "    # Grab next level links.\n",
    "    second_level_links = links_li(top_link, 'report/')\n",
    "    \n",
    "#     # TEST\n",
    "#     print(second_level_links)\n",
    "#     break\n",
    "    \n",
    "    # Go to each link.\n",
    "    for second_link in second_level_links:\n",
    "        scraped_info = scraper(second_link) # dict\n",
    "        info.update(scraped_info) # Merges dict\n",
    "        \n",
    "        # Take a break to not hammer the site.\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            print('{:.2f} min elapsed'.format((time.time() - start_time)/ 60))\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print('Completed download of {} records in {:.2f} minutes!'.format(count, (time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "current_datetime = datetime.datetime.now()\n",
    "output_name = 'fixmystreet_list_au_' + current_datetime.strftime(\"%Y-%m-%d_%H-%M\") + '.csv'\n",
    "\n",
    "pd.DataFrame.from_dict(info, orient='index').reset_index(drop=True).to_csv(output_name, index_label='index')\n",
    "# pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
